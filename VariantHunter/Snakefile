configfile: "config.yaml"

import os
import re


SAMPLE_NAMES, = glob_wildcards(config["seq_path"] + "{sample_name}_R1.trim.fq.gz")

#rule all:
#    input:
#        expand("intermediate_data/bam/{sample_name}.bam", sample_name=SAMPLE_NAMES)

rule all:
    input:
        "intermediate_data/bam/DTU_2016_62_2_MG_DEU-27_1.bam"




#rule all:
#    input:
#        expand("intermediate_data/bam_sorted/{sample_name}.sort.bam", sample_name=SAMPLE_NAMES)

rule prepare_ref:
    input:
        config["ref_path"]
    output:
        fasta="intermediate_data/ref/ref.fasta",
        "intermediate_data/ref/ref.comp.b",
        "intermediate_data/ref/ref.fasta.fai",
        "intermediate_data/ref/ref.name"
    shell:
        '''
        cp {input} {output.fasta;
        sed -i -e 's/(/_/g' -e 's/)/_/g' -e 's/|/_/g' -e 's/:/_/g' -e 's/+/_/g' -e 's/-/_/g' {output.fasta};
        kma index -i {output.fasta} -o intermediate_data/ref/ref;
        samtools faidx {output.fasta}
        '''

rule make_gene_name_conversion_file:
    input:
        original_fasta=config["ref_path"],
        new_names="intermediate_data/ref/ref.name"
    output:
        old_names=temp("temp_files/ref.old_name.txt"),
        converstion_file="output/metadata/gene_name_conversion.tsv"
    shell:
        '''
        grep -e ">" {input.original_fasta} | sed 's/>//g'  > {output.old_names};
        paste {output.old_names} {input.new_names} > {output.converstion_file}
        '''








#rule make_gene_name_id_number_conversion_file:
#    input:
#        "intermediate_data/ref/ref.name"
#    output:
#        "intermediate_data/metaintermediate_data/gene_name_id_number_conversion_file.tsv"
#    run:
#        variant_meta = open("intermediate_data/metaintermediate_data/gene_name_id_number_conversion_file.tsv", 'w')
#        with open("intermediate_data/ref/ref.name") as name_file:
#            names = [row.strip() for row in name_file]
#        for name in names:
#            line = str(names.index(name)) + "\t" + name
#            print(line, file =variant_meta)
#        variant_meta.close()




# Map samples to resfinder database, to create consensus fasta and other files
rule kma:
    input:
        ref="intermediate_data/ref/ref.comp.b"
    output:
        bam="intermediate_data/bam/{sample_name}.bam",
        res_file="intermediate_data/kma_res/{sample_name}.res"
    params:
        S=config["seq_path"] + "{sample_name}_R1.singletons.fq.gz",
        out_dir="intermediate_data/kma_res/{sample_name}",
        PE=config["seq_path"] + "{sample_name}_R*.trim.fq.gz",
        ref="intermediate_data/ref/ref"
    shell:
        "kma -i {params.S} -ipe {params.PE} -o {params.out_dir} -t_db {params.ref} "
        " -1t1 -sam 2096 -ConClave 2 -cge -nc -nf | samtools view -bS > {output.bam}"


rule samtools_sort_index_bam_and_coverage:
    input:
        bam="intermediate_data/bam/{sample_name}.bam"
    output:
        sort="intermediate_data/bam_sorted/{sample_name}.sort.bam",
        bai="intermediate_data/bam_sorted/{sample_name}.sort.bam.bai",
        cov="intermediate_data/coverage/{sample_name}.cov"
    params:
        min_BQ=config["minimum_base_Q"]        # Minimum base quality for a base to be considered by samtools coverage
    shell:
        '''
        samtools sort -o {output.sort} {input.bam};
        samtools index {output.sort};
        samtools coverage --no-header --min-BQ {params.min_BQ}  {output.sort} > {output.cov}
        '''


rule bcftools_mpileup_call:
    input:
        bam="intermediate_data/bam_sorted/{sample_name}.sort.bam",
        ref="intermediate_data/ref/ref.fasta"
    output:
        "intermediate_data/vcf/raw/{sample_name}.raw.vcf.gz"
    params:
        max_dp="--max-depth 1000000",  # At a position, read maximally INT reads per input file.
        MQ="--min-MQ 10",  # Minimum mapping quality for an alignment to be used. 
        BQ=config["minimum_base_Q"],  #Minimum base quality for a base to be considered.
        p="--pval-threshold 0.5"     # Accept variant into raw vcf if the frequency of the ref allele  <= p

    shell:
        "bcftools mpileup -Ou -f {input.ref} {params.max_dp} {params.MQ} --min-BQ {params.BQ} --count-orphans "
        " -a INFO/AD  {input.bam} | "
        "bcftools call -c {params.p} --ploidy 1 -Oz -o {output}"

        
rule bcftools_norm_vcf:
    input:
        vcf_raw="intermediate_data/vcf/raw/{sample_name}.raw.vcf.gz",
        ref="intermediate_data/ref/ref.fasta"
    output:
        raw_index="intermediate_data/vcf/raw/{sample_name}.raw.vcf.gz.csi",
        norm_vcf="intermediate_data/vcf/norm/{sample_name}.norm.vcf.gz"
    shell:
        '''
        bcftools index -o {output.raw_index} {input.vcf_raw};
        bcftools norm --rm-dup both -f {input.ref} {input.vcf_raw} -Oz -o {output.norm_vcf}
        '''
        
rule bcftools_filter_vcf:
    input:
        vn="intermediate_data/vcf/norm/{sample_name}.norm.vcf.gz"
    output:
        vf="intermediate_data/vcf/filter_norm/{sample_name}.filter.norm.vcf.gz",
        vf_index="intermediate_data/vcf/filter_norm/{sample_name}.filter.norm.vcf.gz.csi"
    params:
        min_AD=config["minimum_ALT_AD"],   # minimum ALT allele depth to be accepted.
        min_AF=config["minimum_ALT_AF"]    # minimum ALT allele freq to be accepted.
    shell:
        '''
        bcftools filter 
         -i 'INFO/DP!=0 && (ALT="." || (AD[1] = max(AD) 
         && AD[1] >= {params.min_AD} && AD[1] >= (INFO/DP*{params.min_AF})))'
         {input.vn} -Oz -o {output.vf};
        bcftools index -o {output.vf_index} {output.vf}
        '''
rule bcftools_consensus_raw:
    input:
        vf="intermediate_data/vcf/filter_norm/{sample_name}.filter.norm.vcf.gz",
        vf_index="intermediate_data/vcf/filter_norm/{sample_name}.filter.norm.vcf.gz.csi",
        ref="intermediate_data/ref/ref.fasta"
    output:
        raw_consensus="intermediate_data/sample_consensus/raw/{sample_name}.raw.fasta"
    shell:
        '''
        bcftools consensus --absent N  -f {input.ref} {input.vf} > {output.raw_consensus}
        '''
# --absent N means Positions missin from vcf will be N.


# Extract the high coverage consensus sequenses
rule consensus_high_cov:
    input:
        r_c="intermediate_data/sample_consensus/raw/{sample_name}.raw.fasta",
        cov="intermediate_data/coverage/{sample_name}.cov"
    output:
        hc_c="intermediate_data/sample_consensus/high_cov/{sample_name}.hc.fasta",
        hc_list="intermediate_data/coverage/{sample_name}.hc_list"
    params:
        min_cov=config["min_template_cov"]    # minimum template coverage in percent to accept consensus
    shell:
        '''
        awk '$6 >= {params.min_cov}' {input.cov} | awk '{{print $1}}'  > {output.hc_list};
        seqkit grep -n -f {output.hc_list} {input.r_c} > {output.hc_c} 
        '''
# awk -F'>' 'NR==FNR{{ids[$0]; next}} NF>1{{f=($2 in ids)}} f' {output.hc_list} {input.r_c} > {output.hc_c} 

rule perl_sample_id_to_header_sample_fasta:
    input: 
        "intermediate_data/sample_consensus/high_cov/{sample_name}.hc.fasta"
    output:
        "intermediate_data/sample_consensus/high_cov_id/{sample_name}.hc.id.fasta"
    params:
         sample_id= lambda wildcards: wildcards.sample_name
  
    shell:
        '''   
        perl -p -e 's/^>/>{params.sample_id}+/g' {input}  > {output}
        '''     
# sample_id= lambda wildcards: re.search('DTU_[0-9]{4}_([0-9]+)_', wildcards.sample_name).group(1)
#perl -p -e 's/^>/>{params.sample_id}_/g' {input}  > {output}



# cat all variant fastas together 
rule cat_variant_fastas:
    input:
        gene_fasta=expand("intermediate_data/sample_consensus/high_cov_id/{sample_name}.hc.id.fasta", sample_name=SAMPLE_NAMES)

    output:
        "intermediate_data/pre_cluster_data_prep/sample_consensus_sequenses.fasta"
    shell:
        '''
        cat intermediate_data/sample_consensus/high_cov_id/*.hc.id.fasta   >   {output}
        '''

# Find relavant ref genes
rule cat_make_list_of_genes_found_in_samples:
    input:
        expand("intermediate_data/coverage/{sample_name}.hc_list", sample_name=SAMPLE_NAMES)
    output:
        "intermediate_data/pre_cluster_data_prep/gene_names_from_consensus_files.txt"
    shell:
        '''
        cat intermediate_data/coverage/*.hc_list | sort -u > {output}  
        '''


# Make fasta of relevant ref seq and add an R- in the beginning of the 
#reference fasta headers to identify that the sequence is a ref.
rule seqkit_perl_make_fasta_of_ref_seqs_for_genes_in_samples:
    input:
        ref="intermediate_data/ref/ref.fasta",
        list="intermediate_data/pre_cluster_data_prep/gene_names_from_consensus_files.txt"    
    output:
        "intermediate_data/pre_cluster_data_prep/refs_for_genes_found_in_samples.fasta"
    shell:
        '''        
        seqkit grep -n -f {input.list} {input.ref} | perl -p -e 's/^>/>Ref+/g' > {output}
        '''


# append refgenes to sample genes

rule cat_sample_and_ref:
    input:
        gene_fasta= "intermediate_data/pre_cluster_data_prep/sample_consensus_sequenses.fasta",
        ref_fasta= "intermediate_data/pre_cluster_data_prep/refs_for_genes_found_in_samples.fasta"

    output:
        "intermediate_data/pre_cluster_data_prep/ref_plus_consensus_seq.fasta"
    shell:
        '''
        cat {input.gene_fasta} {input.ref_fasta}  >   {output}
        '''

# Merge dublicate sequences so that all sequences are unique. Also merges the headers
rule rm_dup:
    input:
        "intermediate_data/pre_cluster_data_prep/ref_plus_consensus_seq.fasta"
    output:
        "intermediate_data/pre_cluster_data_prep/ref_plus_consensus_seq_no_dub.fasta"
    shell:
        "python3 scripts/DupRemover.py -i {input} -o {output}"

# remove seqs with with a certain percentage of Ns and under a certain length
rule seq_cleaner:
    input:
        "intermediate_data/pre_cluster_data_prep/ref_plus_consensus_seq_no_dub.fasta"
    output:
        "intermediate_data/pre_cluster_data_prep/clear_ref_plus_consensus_seq_no_dub.fasta"
    params:
        l="0",  # minimum length of sequences
        p=config["percentage_unknown_nuc_allowed"]   # max percentage Ns allowed in sequenses
    shell:
        "cd intermediate_data/pre_cluster_data_prep;"
        "python3 ../../scripts/sequence_cleaner.py  ref_plus_consensus_seq_no_dub.fasta {params.l} {params.p}"


rule python3_create_variant_ID_and_meta_file:
    input:
        fasta="intermediate_data/pre_cluster_data_prep/clear_ref_plus_consensus_seq_no_dub.fasta"
    output:
        fasta_id="intermediate_data/pre_cluster_data_prep/clear_ref_plus_consensus_seq_no_dub_ID.fasta",
        tsv="intermediate_data/metaintermediate_data/variant_metafile.tsv"
    run:
        variant_meta = open("intermediate_data/metaintermediate_data/variant_metafile.tsv", 'w')
        fasta_id_file = open("intermediate_data/pre_cluster_data_prep/clear_ref_plus_consensus_seq_no_dub_ID.fasta",'w')
        fasta_file = open("intermediate_data/pre_cluster_data_prep/clear_ref_plus_consensus_seq_no_dub.fasta","r")
        variant_count = {}
        for line in fasta_file:
            if line.startswith(">"):
                line = line.strip(">")
                last_entry = line.split("|")[-1]
                gene_name= last_entry.split("+", 1)[1].strip()  
                if last_entry.startswith("Ref"):
                    varname = gene_name + "_R"
                else:
                    if gene_name not in variant_count:
                        variant_count[gene_name] = 1
                    else: 
                        variant_count[gene_name] += 1
                    varname = gene_name + "_v" + str(variant_count[gene_name])                    
                new_header= ">" + varname
                print(new_header, file =fasta_id_file)
                sample_and_gene_list = line.split("|") # split the string into the different headers
                sample_list = [i.split("+", 1)[0] for i in sample_and_gene_list] # make list of sample IDs
                metafile_line = varname + "\t" + ' '.join(sample_list)
                print(metafile_line, file =variant_meta)
            else:
                print(line.strip(), file =fasta_id_file)
        variant_meta.close()        
        fasta_id_file.close()
        fasta_file.close()

######## Clusters ###############################################################################

rule cd_hit:
    input:
        "intermediate_data/pre_cluster_data_prep/clear_ref_plus_consensus_seq_no_dub_ID.fasta"
    output:
        "intermediate_data/cd-hit/genes.clstr",
        "intermediate_data/cd-hit/genes"
    params:
        i= config["cluster_identity_cd_hit"],
        w= config["cluster_wordsize_cd_hit"]
    shell:
        '''
        cd-hit-est -i {input} -o intermediate_data/cd-hit/genes -d 0 -c {params.i} -n {params.w} -d 0 -M 16000 -T 8 -s 0.3 -sc 1 -g 1 -mask N
        '''

rule make_cluster_fastas:
    input:
        cluster="intermediate_data/cd-hit/genes.clstr",
        fasta="intermediate_data/pre_cluster_data_prep/clear_ref_plus_consensus_seq_no_dub_ID.fasta"
    output:
        dynamic("intermediate_data/cluster/fastas/{cluster_id}")
    params:
        outdir="intermediate_data/cluster/fastas",
        min_cluster_size = config["minimum_cluster_size"]
    shell:
        '''
        perl scripts/make_multi_seq.pl {input.fasta} {input.cluster} {params.outdir} {params.min_cluster_size}
        '''


rule python3_seqkit_add_refs_to_clusters_that_lost_refs_in_clustering:
    input:
        c_fasta_raw="intermediate_data/cluster/fastas/{cluster_id}",
        all_seq_fasta="intermediate_data/pre_cluster_data_prep/clear_ref_plus_consensus_seq_no_dub_ID.fasta"
    output:
        list="intermediate_data/cluster/ref_add_list/{cluster_id}.ref_add.txt",
        complete_c_fasta="intermediate_data/cluster/fastas/{cluster_id}.fasta"
    run:
        fasta_in = open(input[0],"r")
        ref_set = set()
        var_set = set()
        for line in fasta_in:
            if line.startswith(">"):    # if it is a fasta header
                row = line[1:].strip()   # remove the >
                gene_name = "_".join(row.split("_")[0:-1]) # extract gene name
                if row.split("_")[-1] is "R":  # if it is a ref sequence
                    ref_set.add(gene_name)     # save the gene name in ref set
                else:                          # if it is not a ref sequence
                    var_set.add(gene_name)     # save the gene name in variant set              
        fasta_in.close()
        missing_ref = var_set.difference(ref_set)  # find the var genes that does not have their ref sequence in the cluster

        list_out= open(output[0],"w")
        for item in missing_ref:
            print(item + "_R", file=list_out)            
        list_out.close()
        #if len(missing_ref) == 0:  # if there is no missing ref seq then just copy the input fasta
        shell("cp {input.c_fasta_raw}  {output.complete_c_fasta}")
        if len(missing_ref) > 0:  # if there are missing ref fastas, then add the missing ones to the input fasta
            shell("seqkit grep --line-width 0 -n -f {output.list} {input.all_seq_fasta} >> {output.complete_c_fasta}") 


# Perform multible alignment of cluster fastas
rule mafft:
    input:
        "intermediate_data/cluster/fastas/{cluster_id}.fasta"
    output:
        ali_fasta="intermediate_data/cluster/ali/fasta/{cluster_id}.fasta",
        tab="intermediate_data/cluster/ali/tab/{cluster_id}.tab"
    shell:
        "mafft --auto {input} > {output.fasta};"
        "seqkit fx2tab {output.ali_fasta} > {output.tab}"



rule tree:
    input:
        "intermediate_data/cluster/ali/fasta/{cluster_id}.fasta"
    output:
        "output/tree/{cluster_id}.tree"
    params:
        tree_software=config["Tree_software"]
    run:
        if config["Tree_software"].lower() == "fasttree":
            shell("FastTree -gtr -nt  {input} > {output}")
        else:
            print("Error, no treesoftware chosen. Chose between 'fasttree'")

rule nexus:
    input:
        "intermediate_data/cluster_ali/{cluster_id}.fasta"
    output:
        "intermediate_data/paup/{cluster_id}.ali_nexus"
    shell:
        "seqret {input} {output} -osformat2=nexus"


rule paup_block:
    input:
        "intermediate_data/paup/{cluster_id}.ali_nexus"
    output:
        "intermediate_data/paup/{cluster_id}.paup_block"
    shell:
        '''
        echo "Begin paup;" >> {output};
        echo "set autoclose=yes warntree=no warnreset=no;" >> {output};
        echo "execute {wildcards.cluster_id}.ali_nexus;" >> {output};
        echo "set increase=auto;" >> {output};
        echo "set criterion=distance;" >> {output};
        echo "dset distance=hky85 objective=lsfit power=0 missdist=ignore;" >> {output};
        echo "saveDist file={wildcards.cluster_id}.dist.tsv;" >> {output};
        echo "quit;" >> {output};
        echo "end;" >> {output}
        '''

#        echo "hsearch start=nj swap=nni;" >> {output};
#        echo "savetrees from=1 to=1 file={wildcards.cluster_id}.tree.nexus brlens=yes;" >> {output};

rule paup:
    input:
        paup_block="intermediate_data/paup/{cluster_id}.paup_block",
        alignment="intermediate_data/paup/{cluster_id}.ali_nexus"
    output:
        dist="intermediate_data/paup/{cluster_id}.dist.tsv"
    shell:
        '''
        cd intermediate_data/paup;
        paup {wildcards.cluster_id}.paup_block
        '''















