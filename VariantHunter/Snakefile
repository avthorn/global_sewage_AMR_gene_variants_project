configfile: "config.yaml"

import os

# open sample name file and read the content in a list
with open(config["sample_name_path"], 'r') as file:
    SAMPLE_NAMES = [sample_name.rstrip() for sample_name in file.readlines()]

# Change input to desired output
rule all:
    input:
        dynamic("output/6_clusters/trees/{cluster_id}.tree")        

## Reference ##################################################################

# Remove illegal characters from reference names and index the reference
rule prepare_index_reference:
    input:
        config["ref_path"]
    output:
        fasta="output/1_reference/ref.fasta",
        c="output/1_reference/ref.comp.b",
        fai="output/1_reference/ref.fasta.fai",
        name="output/1_reference/ref.name",
        old_names=temp("output/1_reference/ref.old_name.txt"),
        converstion_file="output/1_reference/gene_name_conversion.tsv"
    shell:
        '''
        cp {input} {output.fasta};
        sed -i -e "s/[;|:|,| |(|)|'|+]/_/g" -e "s/-/_/g" -e "s/\//_/g" -e "s/\./_/g"  -e "s/|/_/g"  {output.fasta};
        sed -i 's/__/_/g'  {output.fasta};
        sed -i 's/__/_/g'  {output.fasta};

        kma index -i {output.fasta} -o output/1_reference/ref;
        samtools faidx {output.fasta};
        grep -e ">" {input} | sed 's/>//g'  > {output.old_names};
        paste {output.old_names} {output.name} > {output.converstion_file}

        '''

## Mapping #########################################################################

# Map samples to resfinder database, to create consensus fasta and other files
rule kma:
    input:
        ref="output/1_reference/ref.comp.b"
    output:
        bam_raw=temp("output/2_mapping/bam_raw/{sample_name}.bam"),
        res_file="output/2_mapping/{sample_name}.res",
        bam_sort="output/2_mapping/{sample_name}.bam",
        bai="output/2_mapping/{sample_name}.bam.bai",
        cov="output/2_mapping/{sample_name}.cov"
    params:
        S=config["seq_path"] + "{sample_name}" + config["singleton_suf"],
        out_dir="output/2_mapping/{sample_name}",
        PE=config["seq_path"] + "{sample_name}" + config["for_rev_suf"],
        ref="output/1_reference/ref",
        min_BQ=config["minimum_base_Q"],        # Minimum base quality for a base to be considered by samtools coverage
        conclave=config["conclave_verson"],
        ali_settings=config["alignment_setting_for_kma"]
    shell:
        "kma -i {params.S} -ipe {params.PE} -o {params.out_dir} -t_db {params.ref} "
        " -1t1 -sam 2096 -ConClave {params.conclave} {params.ali_settings} -nc -nf | samtools view -bS > {output.bam_raw};"
        "samtools sort -o {output.bam_sort} {output.bam_raw};"
        "samtools index {output.bam_sort};"
        "samtools coverage --no-header --min-BQ {params.min_BQ}  {output.bam_sort} > {output.cov}"

## vcf ################################################################################

rule bcftools_mpileup_call:
    input:
        bam="output/2_mapping/{sample_name}.bam",
        ref="output/1_reference/ref.fasta"
    output:
        vcf="output/3_vcf/1_raw/{sample_name}.raw.vcf.gz"
    params:
        max_dp="--max-depth 1000000",  # At a position, read maximally INT reads per input file. 
        MQ=config["minimum_map_Q"],  # Minimum mapping quality for an alignment to be used. 
        BQ=config["minimum_base_Q"],  #Minimum base quality for a base to be considered.
        p=config["p_value"],     # Ignore variant if the frequency of the ref allele  >= p
        ploidy=config["ploidy"],
        orphan=config["orphan_reads"]

    shell:
        "bcftools mpileup -Ou -f {input.ref} {params.max_dp} --min-MQ {params.MQ} --min-BQ {params.BQ} {params.orphan} "
        " -a INFO/AD  {input.bam} | "
        "bcftools call -c -p {params.p} --ploidy {params.ploidy} -Oz -o {output.vcf};"
        
# Normalize and left-align indels and remove dublicate entries of the same indel.

rule bcftools_norm_vcf:
    input:
        vcf_raw="output/3_vcf/1_raw/{sample_name}.raw.vcf.gz",
        ref="output/1_reference/ref.fasta"
    output:
        norm_vcf="output/3_vcf/2_norm/{sample_name}.norm.vcf.gz"
    shell:
        '''
        bcftools norm --rm-dup indels -f {input.ref} {input.vcf_raw} -Oz -o {output.norm_vcf}
        '''
        
rule bcftools_filter_vcf:
    input:
        vn="output/3_vcf/2_norm/{sample_name}.norm.vcf.gz"
    output:
        vf="output/3_vcf/3_filter_norm/{sample_name}.filter.norm.vcf.gz",
        vf_index="output/3_vcf/3_filter_norm/{sample_name}.filter.norm.vcf.gz.csi"
    params:
        min_AD=config["minimum_ALT_AD"],   # minimum ALT allele depth to be accepted.
        min_AF=config["minimum_ALT_AF"],    # minimum ALT allele freq to be accepted.
        min_strand=config["minimum_ALT_strand_prop_forward_and_reverse"]
    shell:
        '''
        bcftools filter -i '(ALT="." || (DP4[2] >= AD[1]*{params.min_strand} && \
        DP4[3]  >= AD[1]*{params.min_strand} && AD[1] = max(AD) && AD[1] >= {params.min_AD} && \
        AD[1] >= (INFO/DP*{params.min_AF} )))' {input.vn} -Oz -o {output.vf};
        bcftools index -o {output.vf_index} {output.vf}
        '''

## Sample consensus sequences ##########################################

rule bcftools_consensus_raw:
    input:
        vf="output/3_vcf/3_filter_norm/{sample_name}.filter.norm.vcf.gz",
        vf_index="output/3_vcf/3_filter_norm/{sample_name}.filter.norm.vcf.gz.csi",
        ref="output/1_reference/ref.fasta"
    output:
        raw_consensus="output/4_sample_consensus/raw/{sample_name}.raw.fasta"
    shell:
        '''
        bcftools consensus --absent N  -f {input.ref} {input.vf} > {output.raw_consensus}
        '''


# Extract the high coverage consensus sequenses
rule consensus_high_cov:
    input:
        r_c="output/4_sample_consensus/raw/{sample_name}.raw.fasta",
        cov="output/2_mapping/{sample_name}.cov"
    output:
        hc_c=temp("output/4_sample_consensus/high_cov/{sample_name}.hc.fasta"),
        hc_list=temp("output/4_sample_consensus/high_cov/{sample_name}.hc_list")
    params:
        min_cov=config["min_template_cov"],    # minimum template coverage in percent to accept consensus
        min_meandepth=config["min_meandepth"]
    run:
        shell("awk '$6 >= {params.min_cov} && $7 >= {params.min_meandepth}' {input.cov} | awk '{{print $1}}'  > {output.hc_list}")
        if os.path.getsize(output[1]) > 0:  # Test that high coverage list file is not empty.
            shell("seqkit grep -n -f {output.hc_list} {input.r_c} > {output.hc_c}")
        else:
            shell("touch {output.hc_c}")    # Create empty fasta file if high coverage list is empty
        

rule perl_sample_id_to_header_sample_fasta:
    input: 
        "output/4_sample_consensus/high_cov/{sample_name}.hc.fasta"
    output:
        "output/4_sample_consensus/high_cov_id/{sample_name}.hc.id.fasta"
    params:
         sample_id= lambda wildcards: wildcards.sample_name
  
    shell:
        '''   
        perl -p -e 's/^>/>{params.sample_id}+/g' {input}  > {output}
        '''     
## Pooled Samples ############################################################################

rule cat_variant_fastas:
    input:
        gene_fasta=expand("output/4_sample_consensus/high_cov_id/{sample_name}.hc.id.fasta", sample_name=SAMPLE_NAMES)
    output:
        temp("output/5_all_samples_pooled/sample_consensus_sequenses.fasta")
    shell:
        '''
        cat output/4_sample_consensus/high_cov_id/*.hc.id.fasta   >   {output}
        '''


# Find Relavant reference genes
# Make fasta of relevant ref seq and add an Ref+ in the beginning of the 
#reference fasta headers to identify that the sequence does not come from a sample but is a ref.
# Then add to the sample fastas.

rule seqkit_perl_make_fasta_of_ref_seqs_for_genes_in_samples:
    input:
        expand("output/4_sample_consensus/high_cov/{sample_name}.hc_list", sample_name=SAMPLE_NAMES),
        gene_fasta= "output/5_all_samples_pooled/sample_consensus_sequenses.fasta",
        ref="output/1_reference/ref.fasta"
    output:
        ref_fasta=temp("output/5_all_samples_pooled/refs_for_genes_found_in_samples.fasta"),
        combi_fasta="output/5_all_samples_pooled/sample_consensus_plus_ref_seqs.fasta",
        list=temp("output/5_all_samples_pooled/gene_names_from_consensus_files.txt")
    shell:
        '''        
        cat output/4_sample_consensus/high_cov/*.hc_list | sort -u > {output.list};
        seqkit grep -n -f {output.list} {input.ref} | perl -p -e 's/^>/>Ref+/g' > {output.ref_fasta};
        cat {input.gene_fasta} {output.ref_fasta}  >   {output.combi_fasta}
        '''


# Merge dublicate sequences so that all sequences are unique. Also merges the headers
rule rm_dup:
    input:
        "output/5_all_samples_pooled/sample_consensus_plus_ref_seqs.fasta"
    output:
        "output/5_all_samples_pooled/sample_consensus_plus_ref_seqs_dublicants_merged.fasta"
    shell:
        "python3 scripts/DupRemover.py -i {input} -o {output}"

# remove seqs with with a certain percentage of Ns 
rule seq_cleaner:
    input:
        "output/5_all_samples_pooled/sample_consensus_plus_ref_seqs_dublicants_merged.fasta"
    output:
        "output/5_all_samples_pooled/clear_sample_consensus_plus_ref_seqs_dublicants_merged.fasta"
    params:
        l="0",  # minimum length of sequences. Set to 0 since coverage is filtered upstreams in the pipeline.
        p=config["percentage_unknown_nuc_allowed"]   # max percentage Ns allowed in sequenses
    shell:
        "cd output/5_all_samples_pooled;"
        "python3 ../../scripts/sequence_cleaner.py  sample_consensus_plus_ref_seqs_dublicants_merged.fasta {params.l} {params.p}"


rule python3_create_variant_ID_and_meta_file:
    input:
        fasta="output/5_all_samples_pooled/clear_sample_consensus_plus_ref_seqs_dublicants_merged.fasta"
    output:
        fasta_id="output/5_all_samples_pooled/final_sequenses_for_clustering.fasta",
        tsv="output/5_all_samples_pooled/final_sequenses_sample_metadata.tsv"
    run:
        variant_meta = open("output/5_all_samples_pooled/final_sequenses_sample_metadata.tsv", 'w')
        fasta_id_file = open("output/5_all_samples_pooled/final_sequenses_for_clustering.fasta",'w')
        fasta_file = open("output/5_all_samples_pooled/clear_sample_consensus_plus_ref_seqs_dublicants_merged.fasta","r")
        variant_count = {}
        for line in fasta_file:
            if line.startswith(">"):
                line = line.strip(">")
                last_entry = line.split("|")[-1]
                gene_name= last_entry.split("+")[1].strip()  
                if last_entry.startswith("Ref"):
                    varname = gene_name + ".R"
                else:
                    if gene_name not in variant_count:
                        variant_count[gene_name] = 1
                    else: 
                        variant_count[gene_name] += 1
                    varname = gene_name + ".v" + str(variant_count[gene_name])                    
                new_header= ">" + varname
                print(new_header, file =fasta_id_file)
                sample_and_gene_list = line.split("|") # split the string into the different headers
                sample_list = [i.split("+", 1)[0] for i in sample_and_gene_list] # make list of sample IDs
                metafile_line = varname + "\t" + ' '.join(sample_list)
                print(metafile_line, file =variant_meta)
            else:
                print(line.strip(), file =fasta_id_file)
        variant_meta.close()        
        fasta_id_file.close()
        fasta_file.close()

## Clusters ###############################################################################

rule cd_hit:
    input:
        "output/5_all_samples_pooled/final_sequenses_for_clustering.fasta"
    output:
        fastas=temp(dynamic("output/6_clusters/cd-hit_fastas/{cluster_id}"))
    params:
        i= config["cluster_identity_cd_hit"],
        w= config["cluster_wordsize_cd_hit"],
        fasta_outdir="output/6_clusters/cd-hit_fastas",
        min_cluster_size = config["minimum_cluster_size"]

    shell:
        '''
        cd-hit-est -i {input} -o output/6_clusters/genes -M 0 -d 0 -c {params.i} -n {params.w} -d 0 -T 8 -s 0.5 -sc 1 -g 1 -mask N;
        rm output/6_clusters/genes;
        perl scripts/make_multi_seq.pl {input} output/6_clusters/genes.clstr {params.fasta_outdir} {params.min_cluster_size};
        rm output/6_clusters/genes.clstr
        '''


rule python3_seqkit_add_refs_to_clusters_that_lost_refs_in_clustering:
    input:
        c_fasta_raw="output/6_clusters/cd-hit_fastas/{cluster_id}",
        all_seq_fasta="output/5_all_samples_pooled/final_sequenses_for_clustering.fasta"
    output:
        list=temp("output/6_clusters/ref_add_lists/{cluster_id}.ref_add.txt"),
        complete_c_fasta="output/6_clusters/fastas/{cluster_id}.fasta"
    run:
        fasta_in = open(input[0],"r")
        ref_set = set()
        var_set = set()
        for line in fasta_in:
            if line.startswith(">"):    # if it is a fasta header
                row = line[1:].strip()   # remove the >
                gene_name = row.split(".")[0] # extract gene name
                if row.split(".")[1] is "R":  # if it is a ref sequence
                    ref_set.add(gene_name)     # save the gene name in ref set
                else:                          # if it is not a ref sequence
                    var_set.add(gene_name)     # save the gene name in variant set              
        fasta_in.close()
        missing_ref = var_set.difference(ref_set)  # find the var genes that does not have their ref sequence in the cluster
        list_out= open(output[0],"w")
        for item in missing_ref:
            print(item + ".R", file=list_out)            
        list_out.close()
        shell("cp {input.c_fasta_raw}  {output.complete_c_fasta}")
        if len(missing_ref) > 0:  # if there are missing ref fastas, then add the missing ones to the input fasta
            shell("seqkit grep --line-width 0 -n -f {output.list} {input.all_seq_fasta} >> {output.complete_c_fasta}") 


# Perform multible alignment of cluster fastas
rule align:
    input:
        fasta="output/6_clusters/fastas/{cluster_id}.fasta"
    output:
        fasta="output/6_clusters/alignments/fasta/{cluster_id}.fasta",
        tab="output/6_clusters/alignments/tab/{cluster_id}.tab",
        nex="output/6_clusters/alignments/nexus/{cluster_id}.nexus"
    shell:
        "mafft --auto {input.fasta} > {output.fasta};"
        "seqkit fx2tab {output.fasta} > {output.tab};"
        "seqret {output.fasta} {output.nex} -osformat2=nexus"


rule tree:
    input:
        "output/6_clusters/alignments/fasta/{cluster_id}.fasta"
    output:
        "output/6_clusters/trees/{cluster_id}.tree"
    shell:
        "FastTree -gtr -nt  {input} > {output}"




