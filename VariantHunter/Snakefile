configfile: "config.yaml"

import os
#import re


# open sample name file and read the content in a list
with open(config["sample_name_path"], 'r') as file:
    SAMPLE_NAMES = [sample_name.rstrip() for sample_name in file.readlines()]



#rule all:
#    input:
#        expand("output/bam/{sample_name}.bam", sample_name=SAMPLE_NAMES)

rule all:
    input:
        "output/pre-clustering_pooled_sample_files/clear_ref_plus_consensus_seq_no_dub.fasta"




# Remove illegal characters from reference names and index the reference

rule prepare_index_reference:
    input:
        config["ref_path"]
    output:
        fasta="output/reference/ref.fasta",
        c="output/reference/ref.comp.b",
        fai="output/reference/ref.fasta.fai",
        name="output/reference/ref.name"
    shell:
        '''
        cp {input} {output.fasta};
        sed -i -e "s/[;|:|,| |(|)|'|+]/_/g" -e "s/-/_/g" -e "s/\//_/g" -e "s/\./_/g"  -e "s/|/_/g"  {output.fasta};
        sed -i 's/__/_/g'  {output.fasta};
        sed -i 's/__/_/g'  {output.fasta};

        kma index -i {output.fasta} -o output/reference/ref;
        samtools faidx {output.fasta}
        '''

# Map samples to resfinder database, to create consensus fasta and other files
rule kma:
    input:
        ref="output/reference/ref.comp.b"
    output:
        bam_raw=temp("output/bam_raw/{sample_name}.bam"),
        res_file="output/mapping/{sample_name}.res",
        bam_sort="output/mapping/{sample_name}.bam",
        bai="output/mapping/{sample_name}.bam.bai",
        cov="output/mapping/{sample_name}.cov"
    params:
        S=config["seq_path"] + "{sample_name}" + config["singleton_suf"],
        out_dir="output/mapping/{sample_name}",
        PE=config["seq_path"] + "{sample_name}" + config["for_rev_suf"],
        ref="output/reference/ref",
        min_BQ=config["minimum_base_Q"],        # Minimum base quality for a base to be considered by samtools coverage
        conclave=config["conclave_verson"],
        ali_settings=config["alignment_setting_for_kma"]
    shell:
        "kma -i {params.S} -ipe {params.PE} -o {params.out_dir} -t_db {params.ref} "
        " -1t1 -sam 2096 -ConClave {params.conclave} {params.ali_settings} -nc -nf | samtools view -bS > {output.bam_raw};"
        "samtools sort -o {output.bam_sort} {output.bam_raw};"
        "samtools index {output.bam_sort};"
        "samtools coverage --no-header --min-BQ {params.min_BQ}  {output.bam_sort} > {output.cov}"



rule bcftools_mpileup_call:
    input:
        bam="output/mapping/{sample_name}.bam",
        ref="output/reference/ref.fasta"
    output:
        vcf="output/vcf/raw/{sample_name}.raw.vcf.gz"
    params:
        max_dp="--max-depth 1000000",  # At a position, read maximally INT reads per input file. 
        MQ=config["minimum_map_Q"],  # Minimum mapping quality for an alignment to be used. 
        BQ=config["minimum_base_Q"],  #Minimum base quality for a base to be considered.
        p=config["p_value"],     # Ignore variant if the frequency of the ref allele  >= p
        ploidy=config["ploidy"],
        orphan=config["orphan_reads"]

    shell:
        "bcftools mpileup -Ou -f {input.ref} {params.max_dp} --min-MQ {params.MQ} --min-BQ {params.BQ} {params.orphan} "
        " -a INFO/AD  {input.bam} | "
        "bcftools call -c -p {params.p} --ploidy {params.ploidy} -Oz -o {output.vcf};"
        
# Normalize and left-align indels and remove dublicate entries of the same indel.

rule bcftools_norm_vcf:
    input:
        vcf_raw="output/vcf/raw/{sample_name}.raw.vcf.gz",
        ref="output/reference/ref.fasta"
    output:
        norm_vcf="output/vcf/norm/{sample_name}.norm.vcf.gz"
    shell:
        '''
        bcftools norm --rm-dup indels -f {input.ref} {input.vcf_raw} -Oz -o {output.norm_vcf}
        '''
        
rule bcftools_filter_vcf:
    input:
        vn="output/vcf/norm/{sample_name}.norm.vcf.gz"
    output:
        vf="output/vcf/filter_norm/{sample_name}.filter.norm.vcf.gz",
        vf_index="output/vcf/filter_norm/{sample_name}.filter.norm.vcf.gz.csi"
    params:
        min_AD=config["minimum_ALT_AD"],   # minimum ALT allele depth to be accepted.
        min_AF=config["minimum_ALT_AF"],    # minimum ALT allele freq to be accepted.
        min_strand=config["minimum_ALT_strand_prop_forward_and_reverse"]
    shell:
        '''
        bcftools filter -i 'INFO/DP!=0 && (ALT="." || (DP4[2] >= AD[1]*{params.min_strand} && \
        DP4[3]  >= AD[1]*{params.min_strand} && AD[1] = max(AD) && AD[1] >= {params.min_AD} && \
        AD[1] >= (INFO/DP*{params.min_AF} )))' {input.vn} -Oz -o {output.vf};
        bcftools index -o {output.vf_index} {output.vf}
        '''
rule bcftools_consensus_raw:
    input:
        vf="output/vcf/filter_norm/{sample_name}.filter.norm.vcf.gz",
        vf_index="output/vcf/filter_norm/{sample_name}.filter.norm.vcf.gz.csi",
        ref="output/reference/ref.fasta"
    output:
        raw_consensus="output/sample_consensus/raw/{sample_name}.raw.fasta"
    shell:
        '''
        bcftools consensus --absent N  -f {input.ref} {input.vf} > {output.raw_consensus}
        '''
# --absent N means Positions missin from vcf will be N.


# Extract the high coverage consensus sequenses
rule consensus_high_cov:
    input:
        r_c="output/sample_consensus/raw/{sample_name}.raw.fasta",
        cov="output/mapping/{sample_name}.cov"
    output:
        hc_c="output/sample_consensus/high_cov/{sample_name}.hc.fasta",
        hc_list=temp("output/mapping/{sample_name}.hc_list")
    params:
        min_cov=config["min_template_cov"]    # minimum template coverage in percent to accept consensus
    shell:
        '''
        awk '$6 >= {params.min_cov}' {input.cov} | awk '{{print $1}}'  > {output.hc_list};
        seqkit grep -n -f {output.hc_list} {input.r_c} > {output.hc_c} 
        '''

rule perl_sample_id_to_header_sample_fasta:
    input: 
        "output/sample_consensus/high_cov/{sample_name}.hc.fasta"
    output:
        "output/sample_consensus/high_cov_id/{sample_name}.hc.id.fasta"
    params:
         sample_id= lambda wildcards: wildcards.sample_name
  
    shell:
        '''   
        perl -p -e 's/^>/>{params.sample_id}+/g' {input}  > {output}
        '''     
## Pool Samples ############################################################################

rule cat_variant_fastas:
    input:
        gene_fasta=expand("output/sample_consensus/high_cov_id/{sample_name}.hc.id.fasta", sample_name=SAMPLE_NAMES)
    output:
        "output/pre-clustering_pooled_sample_files/sample_consensus_sequenses.fasta"
    shell:
        '''
        cat output/sample_consensus/high_cov_id/*.hc.id.fasta   >   {output}
        '''

# Find relavant ref genes
rule cat_make_list_of_genes_found_in_samples:
    input:
        expand("output/mapping/{sample_name}.hc_list", sample_name=SAMPLE_NAMES)
    output:
        temp("output/pre-clustering_pooled_sample_files/gene_names_from_consensus_files.txt")
    shell:
        '''
        cat output/mapping/*.hc_list | sort -u > {output}  
        '''

# Make fasta of relevant ref seq and add an Ref+ in the beginning of the 
#reference fasta headers to identify that the sequence does not come from a sample but is a ref.
# Then add to the sample fastas.

rule seqkit_perl_make_fasta_of_ref_seqs_for_genes_in_samples:
    input:
        gene_fasta= "output/pre-clustering_pooled_sample_files/sample_consensus_sequenses.fasta",
        ref="output/reference/ref.fasta",
        list="output/pre-clustering_pooled_sample_files/gene_names_from_consensus_files.txt"    
    output:
        ref_fasta=temp("output/pre-clustering_pooled_sample_files/refs_for_genes_found_in_samples.fasta"),
        combi_fasta="output/pre-clustering_pooled_sample_files/ref_plus_consensus_seq.fasta"
    shell:
        '''        
        seqkit grep -n -f {input.list} {input.ref} | perl -p -e 's/^>/>Ref+/g' > {output.ref_fasta};
        cat {input.gene_fasta} {output.ref_fasta}  >   {output.combi_fasta}
        '''



# Merge dublicate sequences so that all sequences are unique. Also merges the headers
rule rm_dup:
    input:
        "output/pre-clustering_pooled_sample_files/ref_plus_consensus_seq.fasta"
    output:
        "output/pre-clustering_pooled_sample_files/ref_plus_consensus_seq_no_dub.fasta"
    shell:
        "python3 scripts/DupRemover.py -i {input} -o {output}"

# remove seqs with with a certain percentage of Ns and under a certain length
rule seq_cleaner:
    input:
        "output/pre-clustering_pooled_sample_files/ref_plus_consensus_seq_no_dub.fasta"
    output:
        "output/pre-clustering_pooled_sample_files/clear_ref_plus_consensus_seq_no_dub.fasta"
    params:
        l="0",  # minimum length of sequences. Set to 0 since coverage is filtered upstreams in the pipeline.
        p=config["percentage_unknown_nuc_allowed"]   # max percentage Ns allowed in sequenses
    shell:
        "cd output/pre-clustering_pooled_sample_files;"
        "python3 ../../scripts/sequence_cleaner.py  ref_plus_consensus_seq_no_dub.fasta {params.l} {params.p}"


rule python3_create_variant_ID_and_meta_file:
    input:
        fasta="output/pre-clustering_pooled_sample_files/clear_ref_plus_consensus_seq_no_dub.fasta"
    output:
        fasta_id="output/pre-clustering_pooled_sample_files/clear_ref_plus_consensus_seq_no_dub_ID.fasta",
        tsv="output/metaoutput/variant_metafile.tsv"
    run:
        variant_meta = open("output/metaoutput/variant_metafile.tsv", 'w')
        fasta_id_file = open("output/pre-clustering_pooled_sample_files/clear_ref_plus_consensus_seq_no_dub_ID.fasta",'w')
        fasta_file = open("output/pre-clustering_pooled_sample_files/clear_ref_plus_consensus_seq_no_dub.fasta","r")
        variant_count = {}
        for line in fasta_file:
            if line.startswith(">"):
                line = line.strip(">")
                last_entry = line.split("|")[-1]
                gene_name= last_entry.split("+", 1)[1].strip()  
                if last_entry.startswith("Ref"):
                    varname = gene_name + "_R"
                else:
                    if gene_name not in variant_count:
                        variant_count[gene_name] = 1
                    else: 
                        variant_count[gene_name] += 1
                    varname = gene_name + "_v" + str(variant_count[gene_name])                    
                new_header= ">" + varname
                print(new_header, file =fasta_id_file)
                sample_and_gene_list = line.split("|") # split the string into the different headers
                sample_list = [i.split("+", 1)[0] for i in sample_and_gene_list] # make list of sample IDs
                metafile_line = varname + "\t" + ' '.join(sample_list)
                print(metafile_line, file =variant_meta)
            else:
                print(line.strip(), file =fasta_id_file)
        variant_meta.close()        
        fasta_id_file.close()
        fasta_file.close()

######## Clusters ###############################################################################

rule cd_hit:
    input:
        "output/pre-clustering_pooled_sample_files/clear_ref_plus_consensus_seq_no_dub_ID.fasta"
    output:
        "output/cd-hit/genes.clstr",
        "output/cd-hit/genes"
    params:
        i= config["cluster_identity_cd_hit"],
        w= config["cluster_wordsize_cd_hit"]
    shell:
        '''
        cd-hit-est -i {input} -o output/cd-hit/genes -d 0 -c {params.i} -n {params.w} -d 0 -M 16000 -T 8 -s 0.3 -sc 1 -g 1 -mask N
        '''

rule make_cluster_fastas:
    input:
        cluster="output/cd-hit/genes.clstr",
        fasta="output/pre-clustering_pooled_sample_files/clear_ref_plus_consensus_seq_no_dub_ID.fasta"
    output:
        dynamic("output/cluster/fastas/{cluster_id}")
    params:
        outdir="output/cluster/fastas",
        min_cluster_size = config["minimum_cluster_size"]
    shell:
        '''
        perl scripts/make_multi_seq.pl {input.fasta} {input.cluster} {params.outdir} {params.min_cluster_size}
        '''


rule python3_seqkit_add_refs_to_clusters_that_lost_refs_in_clustering:
    input:
        c_fasta_raw="output/cluster/fastas/{cluster_id}",
        all_seq_fasta="output/pre-clustering_pooled_sample_files/clear_ref_plus_consensus_seq_no_dub_ID.fasta"
    output:
        list="output/cluster/ref_add_list/{cluster_id}.ref_add.txt",
        complete_c_fasta="output/cluster/fastas/{cluster_id}.fasta"
    run:
        fasta_in = open(input[0],"r")
        ref_set = set()
        var_set = set()
        for line in fasta_in:
            if line.startswith(">"):    # if it is a fasta header
                row = line[1:].strip()   # remove the >
                gene_name = "_".join(row.split("_")[0:-1]) # extract gene name
                if row.split("_")[-1] is "R":  # if it is a ref sequence
                    ref_set.add(gene_name)     # save the gene name in ref set
                else:                          # if it is not a ref sequence
                    var_set.add(gene_name)     # save the gene name in variant set              
        fasta_in.close()
        missing_ref = var_set.difference(ref_set)  # find the var genes that does not have their ref sequence in the cluster

        list_out= open(output[0],"w")
        for item in missing_ref:
            print(item + "_R", file=list_out)            
        list_out.close()
        #if len(missing_ref) == 0:  # if there is no missing ref seq then just copy the input fasta
        shell("cp {input.c_fasta_raw}  {output.complete_c_fasta}")
        if len(missing_ref) > 0:  # if there are missing ref fastas, then add the missing ones to the input fasta
            shell("seqkit grep --line-width 0 -n -f {output.list} {input.all_seq_fasta} >> {output.complete_c_fasta}") 


# Perform multible alignment of cluster fastas
rule mafft:
    input:
        "output/cluster/fastas/{cluster_id}.fasta"
    output:
        ali_fasta="output/cluster/ali/fasta/{cluster_id}.fasta",
        tab="output/cluster/ali/tab/{cluster_id}.tab"
    shell:
        "mafft --auto {input} > {output.fasta};"
        "seqkit fx2tab {output.ali_fasta} > {output.tab}"



rule tree:
    input:
        "output/cluster/ali/fasta/{cluster_id}.fasta"
    output:
        "output/tree/{cluster_id}.tree"
    params:
        tree_software=config["Tree_software"]
    run:
        if config["Tree_software"].lower() == "fasttree":
            shell("FastTree -gtr -nt  {input} > {output}")
        else:
            print("Error, no treesoftware chosen. Chose between 'fasttree'")


rule make_gene_name_conversion_file:
    input:
        original_fasta=config["ref_path"],
        new_names="output/reference/ref.name"
    output:
        old_names=temp("temp_files/ref.old_name.txt"),
        converstion_file="output/metadata/gene_name_conversion.tsv"
    shell:
        '''
        grep -e ">" {input.original_fasta} | sed 's/>//g'  > {output.old_names};
        paste {output.old_names} {input.new_names} > {output.converstion_file}
        '''





rule nexus:
    input:
        "output/cluster_ali/{cluster_id}.fasta"
    output:
        "output/paup/{cluster_id}.ali_nexus"
    shell:
        "seqret {input} {output} -osformat2=nexus"


rule paup_block:
    input:
        "output/paup/{cluster_id}.ali_nexus"
    output:
        "output/paup/{cluster_id}.paup_block"
    shell:
        '''
        echo "Begin paup;" >> {output};
        echo "set autoclose=yes warntree=no warnreset=no;" >> {output};
        echo "execute {wildcards.cluster_id}.ali_nexus;" >> {output};
        echo "set increase=auto;" >> {output};
        echo "set criterion=distance;" >> {output};
        echo "dset distance=hky85 objective=lsfit power=0 missdist=ignore;" >> {output};
        echo "saveDist file={wildcards.cluster_id}.dist.tsv;" >> {output};
        echo "quit;" >> {output};
        echo "end;" >> {output}
        '''

#        echo "hsearch start=nj swap=nni;" >> {output};
#        echo "savetrees from=1 to=1 file={wildcards.cluster_id}.tree.nexus brlens=yes;" >> {output};

rule paup:
    input:
        paup_block="output/paup/{cluster_id}.paup_block",
        alignment="output/paup/{cluster_id}.ali_nexus"
    output:
        dist="output/paup/{cluster_id}.dist.tsv"
    shell:
        '''
        cd output/paup;
        paup {wildcards.cluster_id}.paup_block
        '''



